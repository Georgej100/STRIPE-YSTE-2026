# -*- coding: utf-8 -*-
"""Tuner and Tester.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxZDEChxM98VJGk2p72-PB-rY5hpdnHs
"""

train_path = /content/train
test_path = /content/test


%%capture
import os, re
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    import torch; v = re.match(r"[0-9]{1,}\.[0-9]{1,}", str(torch.__version__)).group(0)
    xformers = "xformers==" + ("0.0.33.post1" if v=="2.9" else "0.0.32.post2" if v=="2.8" else "0.0.29.post3")
    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets==4.3.0" "huggingface_hub>=0.34.0" hf_transfer
    !pip install --no-deps unsloth
!pip install transformers==4.56.2
!pip install --no-deps trl==0.22.2

from unsloth import FastModel
import torch

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-12b-it-unsloth-bnb-4bit",
    max_seq_length = 2048,
    load_in_4bit = True,
    #full_finetuning = False,
)

model = FastModel.get_peft_model(
    model,
    finetune_language_layers   = True,  # Should leave on!
    finetune_attention_modules = True,  # Attention good for GRPO
    finetune_mlp_modules       = True,  # SHould leave on always!

    r = 16,           # Larger = higher accuracy, but might overfit
    lora_alpha = 32,  # Recommended alpha == r at least
    lora_dropout = 0,
    bias = "none",
    random_state = 13,
)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "gemma-3",
)

from datasets import load_from_disk

train = load_from_disk(train_path)
test = load_from_disk(test_path)

print(train)
print(test)

from unsloth.chat_templates import standardize_data_formats

train = standardize_data_formats(train)
test = standardize_data_formats(test)

train[1]
test[1]

def formatting_prompts_func(examples):
   convos = examples["messages"]
   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]
   return { "text" : texts, }

train = train.map(formatting_prompts_func, batched = True)
test = test.map(formatting_prompts_func, batched = True)

train[1]["text"]

test[1]["text"]

from trl import SFTTrainer, SFTConfig

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train,
    eval_dataset = test, # Can set up evaluation!
    args = SFTConfig(
        output_dir = "checkpoints",
      dataset_text_field = "text",

      per_device_train_batch_size = 2,
      gradient_accumulation_steps = 4,
      num_train_epochs = 4,

      per_device_eval_batch_size = 1,
      eval_accumulation_steps = 1,

      learning_rate = 8e-6,
      warmup_steps = 40,
      lr_scheduler_type = "cosine",

      optim = "adamw_torch_fused",
      weight_decay = 0.001,

      logging_steps = 10,
      save_strategy = "steps",
      save_steps = 35,

      gradient_checkpointing = True,
      seed = 13,
      report_to = "none",

      max_seq_length = 2048
    ),
)

from unsloth.chat_templates import train_on_responses_only

trainer = train_on_responses_only(
    trainer,
    instruction_part = "<start_of_turn>user\n",
    response_part = "<start_of_turn>model\n",
)

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

stats = trainer.train(resume_from_checkpoint= True)
print(stats)

# @title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

model.save_pretrained("model/4ep")

trainer.evaluate()