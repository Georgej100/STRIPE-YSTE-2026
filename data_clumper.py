# -*- coding: utf-8 -*-
"""Turn 4 Data Clumper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UNG1QvNtdgDQdIXpnKACDBdWCM3qXYkk
"""

input_path = "/content/train3"
output_path = "/content/train3_summary.txt"

"""## Installation"""


%%capture
import os
!pip install --upgrade -qqq uv
if "COLAB_" not in "".join(os.environ.keys()):
    # If you're not in Colab, just use pip install!
    !pip install unsloth vllm synthetic-data-kit==0.0.3
else:
    try: import numpy, PIL; get_numpy = f"numpy=={numpy.__version__}"; get_pil = f"pillow=={PIL.__version__}"
    except: get_numpy = "numpy"; get_pil = "pillow"
    try: import subprocess; is_t4 = "Tesla T4" in str(subprocess.check_output(["nvidia-smi"]))
    except: is_t4 = False
    get_vllm, get_triton = ("vllm==0.9.2", "triton==3.2.0") if is_t4 else ("vllm==0.10.2", "triton")
    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers
    !uv pip install -qqq {get_triton}
    !uv pip install synthetic-data-kit==0.0.3
!uv pip install --no-deps trl==0.22.2


%%capture
import os
!pip install --upgrade -qqq uv
if "COLAB_" not in "".join(os.environ.keys()):
    # If you're not in Colab, just use pip install!
    !pip install unsloth vllm
else:
    try: import numpy, PIL; get_numpy = f"numpy=={numpy.__version__}"; get_pil = f"pillow=={PIL.__version__}"
    except: get_numpy = "numpy"; get_pil = "pillow"
    try: import subprocess; is_t4 = "Tesla T4" in str(subprocess.check_output(["nvidia-smi"]))
    except: is_t4 = False
    get_vllm, get_triton = ("vllm==0.9.2", "triton==3.2.0") if is_t4 else ("vllm==0.10.2", "triton")
    !uv pip install -qqq --upgrade \
        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers
    !uv pip install -qqq {get_triton}
!uv pip install --no-deps trl==0.22.2
!uv pip install "transformers<4.54.0"
!uv pip install torch

"""## Summarize"""

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

from datasets import load_from_disk

train = load_from_disk(input_path)

print(train['messages'][1])

instruction = "You are a C programming expert and know everything about the language. Summarize the input provided while keeping in the main points, structures and ideas. The input was taken from a SFT fine tuning database The summary should be at least 3  lines long. Add fluff to the summary to keep it consise. ONLY SUMMARISE THE SECTION FROM THE ASSISTANT INDICATED BY: 'role': 'assistant', DO NOT REFERNCE OR MENTION AND USER-ASSISTANT INTERACTIONS NOR INCLUDE THEM IN YOUR SUMMARY. The summary should simply summarize the assistant explanantion"
input = train['messages'][1]

from vllm import LLM, SamplingParams

# Load vLLM model (fast!)
llm = LLM(
    model="unsloth/Llama-3.2-3B-Instruct",
    max_model_len=4096,               # FIX
    gpu_memory_utilization=0.9        # use GPU more efficiently
    )
params = SamplingParams(max_tokens=64, temperature=0.1)

messages = train["messages"]   # your dataset column
N = train.num_rows               # <-- FIXED: now N is defined

BATCH = 32                      # increase if you have strong GPU

with open(output_path, "a") as f:
    for i in range(0, N, BATCH):
        batch_msgs = messages[i : i + BATCH]

        # Build prompts
        prompts = [
            alpaca_prompt.format(instruction, msg, "")
            for msg in batch_msgs
        ]

        # vLLM generates the batch
        outputs = llm.generate(prompts, params)

        # Extract text from vLLM result format
        for out in outputs:
            response = out.outputs[0].text.strip()
            f.write(response + "\n")