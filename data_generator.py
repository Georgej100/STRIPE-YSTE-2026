# -*- coding: utf-8 -*-
"""Turn 4 Data Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A31Co91NkvyHWJlIEh8lRtds5y-obTjN

# Data Genrator for test 2
Code from: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb#scrollTo=3_N3iAzT55qA
"""

turn_no = 4
input_path = "/content/train3_summary.txt"
input_name = "C"
train_out_path = f"/content/output/train{turn_no}"

"""### Installation"""


%%capture
import os
!pip install --upgrade -qqq uv
if "COLAB_" not in "".join(os.environ.keys()):
    # If you're not in Colab, just use pip install!
    !pip install unsloth vllm synthetic-data-kit==0.0.3
else:
    try: import numpy, PIL; get_numpy = f"numpy=={numpy.__version__}"; get_pil = f"pillow=={PIL.__version__}"
    except: get_numpy = "numpy"; get_pil = "pillow"
    try: import subprocess; is_t4 = "Tesla T4" in str(subprocess.check_output(["nvidia-smi"]))
    except: is_t4 = False
    get_vllm, get_triton = ("vllm==0.9.2", "triton==3.2.0") if is_t4 else ("vllm==0.10.2", "triton")
    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers
    !uv pip install -qqq {get_triton}
    !uv pip install synthetic-data-kit==0.0.3
!uv pip install transformers==4.56.2
!uv pip install --no-deps trl==0.22.2


#@title Colab Extra Install { display-mode: "form" }
%%capture
import os
!pip install --upgrade -qqq uv
if "COLAB_" not in "".join(os.environ.keys()):
    # If you're not in Colab, just use pip install!
    !pip install unsloth vllm
else:
    try: import numpy, PIL; get_numpy = f"numpy=={numpy.__version__}"; get_pil = f"pillow=={PIL.__version__}"
    except: get_numpy = "numpy"; get_pil = "pillow"
    try: import subprocess; is_t4 = "Tesla T4" in str(subprocess.check_output(["nvidia-smi"]))
    except: is_t4 = False
    get_vllm, get_triton = ("vllm==0.9.2", "triton==3.2.0") if is_t4 else ("vllm==0.10.2", "triton")
    !uv pip install -qqq --upgrade \
    unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers
    !uv pip install -qqq {get_triton}
    !uv pip install transformers==4.56.2
!uv pip install --no-deps trl==0.22.2

"""# Setup generator

## Load model
This model is used to take the inputed data and trun it into QA pairs we can use if SFT
"""

from unsloth.dataprep import SyntheticDataKit

generator = SyntheticDataKit.from_pretrained(
    # Choose any model from https://huggingface.co/unsloth
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length = 2048, # Longer sequence lengths will be slower!
)

"""## Generate QA Pairs + Auto clean data
We now use synthetic data kit for question answer pair generation:
"""

generator.prepare_qa_generation(
    output_folder = "data", # Output location of synthetic data
    temperature = 0.7, # Higher temp makes more diverse datases
    top_p = 0.95,
    overlap = 64, # Overlap portion during chunking
    max_generation_tokens = 600, # Can increase for longer QA pairs
)

"""Check if it succeeded:"""

!synthetic-data-kit system-check

"""# Document Parsing
We then take the C standard documentation and turn it into chunks. These smaller portions are then processed
"""

!synthetic-data-kit \
    -c synthetic_data_kit_config.yaml \
    ingest "/content/train3_summary.txt"

# Truncate document
filenames = generator.chunk_data(f"data/output/train3_summary.txt")
print(len(filenames), filenames[:3])

"""We get 53 chunks. We then convert these chunks into 25 QA pairs per chunk. Their size is dependent on the max lentght set above"""

import time
# Process 3 chunks for now -> can increase but slower!
for filename in filenames:
    !synthetic-data-kit \
        -c synthetic_data_kit_config.yaml \
        create {filename} \
        --num-pairs 25 \
        --type "qa"
    time.sleep(2) # Sleep some time to leave some room for processing

"""We now convert the generated datasets into QA formats so we can load it for finetuning:"""

qa_pairs_filenames = [
    f"data/generated/train3_summary_{i}_qa_pairs.json"
    for i in range(len(filenames))
]
for filename in qa_pairs_filenames:
    !synthetic-data-kit \
        -c synthetic_data_kit_config.yaml \
        save-as {filename} -f ft

"""Let's load up the data and see what the synthetic data looks like!"""

from datasets import Dataset
import pandas as pd
final_filenames = [
    f"data/final/train3_summary_{i}_qa_pairs_ft.json"
    for i in range(len(filenames))
]
conversations = pd.concat([
    pd.read_json(name) for name in final_filenames
]).reset_index(drop = True)

dataset = Dataset.from_pandas(conversations)

dataset[0]

dataset[1]

dataset[350]

"""# Shuffle and Split dataset"""

dataset = dataset.shuffle(seed=13)

fraction = 890 / dataset.num_rows
print(fraction)
culled_dataset, temp = dataset.train_test_split(train_size=fraction).values()
print(culled_dataset)

"""## Export dataset"""

culled_dataset.save_to_disk("/content/output/datasets/4/train")

"""Finally free vLLM process to save memory and to allow for finetuning!"""

generator.cleanup()